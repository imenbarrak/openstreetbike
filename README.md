# openstreetbike
kleine ETL-Pipeline, Daten sollen bereinigt, transformiert und in relationale Datenbank gespeichert werden. Prozess wird automatisiert.

# Ãœbungsaufgabe Work in Progress

## ğŸ› ï¸ Projekt: "OpenStreetBike â€“ Fahrradstationsdaten verarbeiten"
### ğŸ§  Ziel:
Baue eine kleine ETL-Pipeline, die Ã¶ffentlich verfÃ¼gbare Daten von Fahrradverleihstationen (z.â€¯B. aus einer europÃ¤ischen Stadt wie Oslo, Paris oder KÃ¶ln) regelmÃ¤ÃŸig verarbeitet. Die Daten sollen bereinigt, transformiert und in eine relationale Datenbank gespeichert werden. ZusÃ¤tzlich wird der Prozess automatisiert.

### ğŸ“¦ Datensatz-Vorschlag
Open-Data-Datensatz Vorschlag:

Name: Oslo City Bike â€“ Station status

Zugriff: Echtzeitdaten via REST-API (JSON)

Daten: VerfÃ¼gbare FahrrÃ¤der, Dockingstations, KapazitÃ¤ten etc.

Alternativ: Stadt KÃ¶ln â€“ Open Data Fahrradverleihstationen

Vorteil: Echtzeitdaten (perfekt fÃ¼r Ãœbung mit Airflow), aber Ã¼berschaubar in der GrÃ¶ÃŸe.

### ğŸ§© Aufgabenstellung (fÃ¼r 1 Woche / ca. 4â€¯h pro Tag)
Tag 1 â€“ Setup & Exploration
- API/CSV-Datenquelle verstehen: Strukturen, Formate, Datenmengen

- Beispiel-Request oder CSV-Datei laden, ersten Ãœberblick verschaffen

- SQLite oder PostgreSQL lokal aufsetzen

- Python-Umgebung vorbereiten (venv, requirements.txt mit requests, pandas, sqlalchemy etc.)

Tag 2 â€“ Extract & Load
- Daten abrufen (API oder CSV)

- In einen DataFrame laden

- Tabellenstruktur fÃ¼r deine Zieldatenbank entwerfen (z.â€¯B. stations, status_logs)

- Ladefunktion schreiben, um die Daten per SQLAlchemy in die DB zu schreiben

Tag 3 â€“ Transform
- Daten bereinigen:

  - Fehlende Werte behandeln

  - Datentypen prÃ¼fen und ggf. umwandeln

  - Zeitformate vereinheitlichen

- Transformation:

  - Neue Spalten ableiten (z.â€¯B. Auslastung = bikes_available / capacity)

  - Daten normalisieren (z.â€¯B. separate Tabellen fÃ¼r statische vs. dynamische Infos)

Tag 4 â€“ Automatisierung mit Prefect oder Airflow
- Workflow-Definition: Extract â†’ Transform â†’ Load

- TÃ¤glicher oder stÃ¼ndlicher Ablauf

- Logging & einfache Fehlerbehandlung integrieren

- Lokales Testen mit Prefect Orion oder Airflow im Docker-Setup

Tag 5 â€“ Erweiterungen & Tests
- Unit Tests fÃ¼r Transformationsfunktionen (z.â€¯B. mit pytest)

- Parametrisierung (z.â€¯B. API-Keys, Pfade, DB-Verbindung) Ã¼ber .env + dotenv

- Logging verbessern (logging-Modul)

Tag 6 â€“ Visualisierung / Abfragen
- Beispielhafte SQL-Abfragen:

  - Stationen mit hÃ¶chster Auslastung

  - Durchschnittliche Auslastung pro Tag

  - Optionale Mini-Visualisierung mit matplotlib oder streamlit (wennâ€™s SpaÃŸ macht)

Tag 7 â€“ Dokumentation & Refactoring
Projektstruktur aufrÃ¤umen (src/, etl/, data/ etc.)

README schreiben:

Projektbeschreibung

Setup-Anleitung

Beispielabfragen

Dockerfile (optional) zur Containerisierung

ğŸ“ Projektstruktur (Vorschlag)
bash
Kopieren
Bearbeiten
bike_etl_project/
â”‚
â”œâ”€â”€ data/                 # Rohdaten oder Beispiel-Downloads
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ pipeline.py       # Hauptpipeline
â”‚
â”œâ”€â”€ dags/                 # Falls du Airflow nutzt
â”œâ”€â”€ tests/                # Pytest-Tests
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run_pipeline.py       # FÃ¼r manuelle AusfÃ¼hrung
ğŸ”§ Tools & Libraries
Python (pandas, requests, sqlalchemy, dotenv, logging, pytest)

Datenbank: SQLite fÃ¼r den Anfang, PostgreSQL optional

Workflow: Prefect (einfacher Einstieg) oder Airflow (Enterprise-ready)

Bonus: streamlit fÃ¼r Mini-Dashboards, docker fÃ¼r Packaging
